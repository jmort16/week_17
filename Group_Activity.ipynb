{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5e81287",
   "metadata": {},
   "source": [
    "##### 1.  Write simple (straightforward) definitions for the following parameters for RandomForestClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and indicate how they correlate with the precision and recall for the basic diabetes model we built in class. You will need to rerun the model multiple times to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc2552",
   "metadata": {},
   "source": [
    "<style>\n",
    "table th {color:blue !important;}\n",
    "table td, table th, table tr {text-align:left !important;}\n",
    "table, th, td {border: 1px solid black !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1472e",
   "metadata": {},
   "source": [
    "<table border=\"1\">\n",
    " <tr>\n",
    "    <td><b style=\"font-size:20px\">Parameter</b></td>\n",
    "    <td><b style=\"font-size:20px\">Definition</b></td>\n",
    "    <td><b style=\"font-size:20px\">Correlation with Precision</b></td>\n",
    "    <td><b style=\"font-size:20px\">Correlation with Recall</b></td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>estimators</td>\n",
    "    <td>The total number of decision tree classifiers used.</td>\n",
    "    <td>Best estimator is 200 for this dataset.  This raised NPV by 1% & PPV by 2%.</td>\n",
    "    <td>This raised specificity by 1% & sensitivity by 4%. Accuracy rose by 2% and scores started dropping at estimators = 275.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>max_depth</td>\n",
    "    <td>The maximum number of \"layers\" or \"rows\" created in the random forest during the process of expanding nodes.</td>\n",
    "    <td>Best max_depth is 9 for this dataset IF all other parameters are set to defaults.  This raised NPV by 1% & PPV by 3%.  With max_depth = 1, NPV plummeted.  As I raised max_depth incrementally, NPV didn't recover until max_depth = 8.</td>\n",
    "    <td>This raised specificity by 1% & sensitivity by 3%. With max_depth = 1, sensitivity plummeted even more than NPV.  As I raised max_depth incrementally, sensitivity didn't recover until max_depth = 8.  Accuracy rose by 2% and all scores were no longer affected at max_depth=11.  Best combo of tuning estimators AND max_depth is estimators = 100 and max_depth = 9.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>min_samples_split</td>\n",
    "    <td>The minimum number of samples required to split an internal node.</td>\n",
    "    <td>Best min_samples_split is 6 for this dataset if all other parameters are set to defaults.  This raised NPV by 1% & PPV by 2%.</td>\n",
    "    <td>This raised specificity by 1% & sensitivity by 3%. Accuracy rose by 1% and scores began to decrease at min_samples_split=8.  Best combo of tuning estimators, max_depth and min_samples_split is to leave estimators and min_samples_split at default values and max_depth = 9.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>min_samples_leaf</td>\n",
    "    <td>The minimum number of samples required to end expansion of nodes, creating a leaf.</td>\n",
    "    <td>Best min_samples_leaf is 4 for this dataset if all other parameters are set to defaults.  This raised NPV by 1% & PPV by 2%.</td>\n",
    "    <td>This raised specificity by 1% & sensitivity by 3%. Accuracy rose by 1% and scores began to decrease at min_samples_leaf=6.  HOWEVER, best combo of tuning estimators, max_depth, min_samples_split, and min_samples_leaf is to leave estimators and min_samples_split at default values, with max_depth = 9 and min_samples_leaf=3.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>min_weight_fraction_leaf</td>\n",
    "    <td>The minimum weight of a node (as a fraction of the sum total of weights of all the input samples) required to be considered a leaf node.</td>\n",
    "    <td>The best setting for this parameter is the default. Raising min_weight_fraction_leaf incrementally caused the NPV to drop and the PPV to increase by larger percentages thatn with the other parameters I have tuned so far.  At min_weight_leaf_fraction = 0.4, my code threw a division by zero error.</td>\n",
    "    <td>Raising min_weight_fraction_leaf incrementally caused the sensitivity to drop and specificity to rise by even larger percentages.  Summarily, this means that there are more false negatives and fewer false positives as min_weight_fraction_leaf is increased, which is not an ideal situation for detecting diabetes.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>max_leaf_nodes</td>\n",
    "    <td>Grow trees with best leaf nodes first (based on reduction in impurity) when a max number of leaf nodes is designated.\n",
    "    <td>Best max_leaf_nodes is 55-58 for this dataset IF all other parameters are set to defaults.  Values lower than 50 caused a 1-2% drop in NPV and rise in PPV.  Max_leaf_node values in the 55 - 58 range raised NPV by 2% and PPV by 5%.  Values above 60 showed a plateau in growth, with 1% growth in NPV and 2% in PPV, generally speaking.</td>\n",
    "    <td>Values lower than 50 caused a 2% rise in specificity and a 2% drop in sensitivity.  Max_leaf_node values in the 55 - 58 range raised the scores 3% and 4% respectively.  Values above 60 showed a rise of just 1% growth in specificity and a 4% in sensitivity.  This growth remained at 1% and 4% respectively for values above 60.  </td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>min_impurity_decrease</td>\n",
    "    <td>Minimum threshhold for splitting a node. If split causes a decrease in impurity at or above this value, the node will be split.</td>\n",
    "    <td>This parameter should be tuned in very small increments, thousandths perhaps. An increase of 0.1 threw an error message. Increasing by 0.01 caused a 3% drop in NPV and a 2% increase in PPV. </td>\n",
    "    <td>Increasing min_impurity_decrease by 0.01 caused a 3% increase in specificity and a 8% decrease in sensitivity.  Just like the min_weight_fraction_leaf parameter, raising min_impurity decrease by a small amount case more false negatives and fewer false positives.  Ideal parameter setting here is the default.</td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td>min_impurity_split</td>\n",
    "    <td>This parameter has been deprecated.</td>\n",
    "    <td>n/a</td>\n",
    "    <td>n/a</td>\n",
    " </tr>\n",
    "<script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7cbf94",
   "metadata": {},
   "source": [
    "In summary, the best combination of parameters tuned for this dataset was to set the max_depth=9 and max_leaf_nodes = 58, while leaving all others set to their defaults.  This gave the following growth:\n",
    "\n",
    "<table border=\"1\">\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\"></b></td>\n",
    "    <td><b style=\"font-size:14px\">Precision</b></td>\n",
    "    <td><b style=\"font-size:14px\">Recall</b></td>\n",
    "    <td><b style=\"font-size:14px\"></b></td> \n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\">0</b></td> \n",
    "    <td>+ 2%</td>\n",
    "    <td>+ 2%</td>\n",
    "    <td></td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\">1</b></td> \n",
    "    <td>+ 5%</td>\n",
    "    <td>+ 5%</td>\n",
    "    <td></td> \n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\"></b></td> \n",
    "    <td><b style=\"font-size:14px\"></b></td>\n",
    "    <td><b style=\"font-size:14px\">Accuracy</b></td>\n",
    "    <td><b style=\"font-size:14px\">+ 3%</b></td>\n",
    " </tr>\n",
    "<script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c02bc",
   "metadata": {},
   "source": [
    "With max_leaf_nodes=55 and all other parameters set to their defaults, the growth was as follows:\n",
    "\n",
    "<table border=\"1\">\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\"></b></td>\n",
    "    <td><b style=\"font-size:14px\">Precision</b></td>\n",
    "    <td><b style=\"font-size:14px\">Recall</b></td>\n",
    "    <td><b style=\"font-size:14px\"></b></td> \n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\">0</b></td> \n",
    "    <td>+ 2%</td>\n",
    "    <td>+ 3%</td>\n",
    "    <td></td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\">1</b></td> \n",
    "    <td>+ 5%</td>\n",
    "    <td>+ 4%</td>\n",
    "    <td></td> \n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\"></b></td> \n",
    "    <td><b style=\"font-size:14px\"></b></td>\n",
    "    <td><b style=\"font-size:14px\">Accuracy</b></td>\n",
    "    <td><b style=\"font-size:14px\">+ 3%</b></td>\n",
    " </tr>\n",
    "<script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
    "</table>\n",
    "\n",
    "These are also very good, but since we are more concerned with minimizing false negatives as opposed to false positives, the former option is better than the latter, in my opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59025b4",
   "metadata": {},
   "source": [
    "### Code used for tuning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47524c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Set-Up\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bbd155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_df = pd.read_csv(\"../wk_13_hmwk/diabetes.csv\")\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3b8039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "X = diabetes_df.drop('Outcome', axis=1)\n",
    "y = diabetes_df['Outcome']\n",
    "\n",
    "# Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "#Standardize\n",
    "sc = StandardScaler()\n",
    "X_train_scaler = sc.fit_transform(X_train)\n",
    "X_test_scaler = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f700be99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       150\n",
      "           1       0.68      0.54      0.60        81\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.73      0.70      0.71       231\n",
      "weighted avg       0.74      0.75      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default Parameters\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth= None, min_samples_split= 2, min_samples_leaf = 1,\n",
    "                            min_weight_fraction_leaf = 0.0, max_leaf_nodes = None, min_impurity_decrease = 0.0, \n",
    "                            random_state=42)\n",
    "rf.fit(X_train_scaler, y_train)\n",
    "y_pred = rf.predict(X_test_scaler)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e124caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84       150\n",
      "           1       0.73      0.59      0.65        81\n",
      "\n",
      "    accuracy                           0.78       231\n",
      "   macro avg       0.76      0.74      0.75       231\n",
      "weighted avg       0.77      0.78      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modified Parameters\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth =9, min_samples_split=2, min_samples_leaf = 1,\n",
    "                            min_weight_fraction_leaf = 0.0, max_leaf_nodes = 58, min_impurity_decrease = 0.0, \n",
    "                            random_state=42)\n",
    "rf.fit(X_train_scaler, y_train)\n",
    "y_pred = rf.predict(X_test_scaler)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdab2a8",
   "metadata": {},
   "source": [
    "##### 2. How does setting bootstrap=False influence the model performance? Note: the default is bootstrap=True. Explain why your results might be so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5707f23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83       150\n",
      "           1       0.71      0.59      0.64        81\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.75      0.73      0.74       231\n",
      "weighted avg       0.77      0.77      0.77       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_depth=None, min_samples_split= 2, min_samples_leaf = 1,\n",
    "                            min_weight_fraction_leaf = 0.0, max_leaf_nodes = None, min_impurity_decrease = 0.0, \n",
    "                            random_state=42, bootstrap = False)\n",
    "rf.fit(X_train_scaler, y_train)\n",
    "y_pred = rf.predict(X_test_scaler)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3919d8",
   "metadata": {},
   "source": [
    "Setting \"bootstrap\" equal to \"False\" gave the following:\n",
    "\n",
    "<table border=\"1\">\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\"></b></td>\n",
    "    <td><b style=\"font-size:14px\">Precision</b></td>\n",
    "    <td><b style=\"font-size:14px\">Recall</b></td>\n",
    "    <td><b style=\"font-size:14px\"></b></td> \n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\">0</b></td> \n",
    "    <td>+ 2%</td>\n",
    "    <td>+ 1%</td>\n",
    "    <td></td>\n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\">1</b></td> \n",
    "    <td>+ 3%</td>\n",
    "    <td>+ 5%</td>\n",
    "    <td></td> \n",
    " </tr>\n",
    " <tr>\n",
    "    <td><b style=\"font-size:14px\"></b></td> \n",
    "    <td><b style=\"font-size:14px\"></b></td>\n",
    "    <td><b style=\"font-size:14px\">Accuracy</b></td>\n",
    "    <td><b style=\"font-size:14px\">+ 2%</b></td>\n",
    " </tr>\n",
    "<script src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f1f32",
   "metadata": {},
   "source": [
    "The growth in these 4 component scores I've been examining (NPV, PPV, specificity, and sensitivity) is nearly as good when setting \"bootstrap\" equal to \"False\" as is it was after I spent a lot of time analyzing and tuning parameters!  This is because when \"bootstrap\" is set equal to \"False,\" the entire dataset is used to build each tree.  If it is set equal to \"True,\" that means that bootstrap samples are used to build the trees.  Using the entire dataset would support the idea that the model will be trained more acutely."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
